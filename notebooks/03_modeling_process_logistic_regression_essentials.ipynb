{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a973072-33cf-4d12-b0b2-0fc7f52bb79e",
   "metadata": {},
   "source": [
    "## **Modeling process**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47cc4f7-2a28-4756-9666-451319c82db3",
   "metadata": {},
   "source": [
    "During modeling phase the metric that will be used to evaluate the performance of the models as well as to facilitate the comparison between them is `balanced accuracy`.  \n",
    "\n",
    "Balanced accuracy is the average of the recall (sensitivity) for each class, treating each class equally regardless of its frequency.   \n",
    "Unlike standard accuracy, which in imbalanced datasets is biased towards the majority class and gives a false impression about the model's performance, balanced accuracy metric provides a more fair view of how well the model performs across all classes and illustrate the overall model performance.\n",
    "\n",
    "For a heavily imbalanced dataset, balanced accuracy will prevent the majority class from dominating the metric, allowing us to see if the model truly performs well on the minority classes, and not only on the majority class.\n",
    "\n",
    "You can find detailed information on evaluation metrics used for both classification and regression tasks in the **[official documentation](https://scikit-learn.org/1.5/modules/model_evaluation.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb423b-8ab4-4d63-92ba-ef45f1db6ee9",
   "metadata": {},
   "source": [
    "### Logistic Regression Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49455fe-962a-4e08-bf62-5528a27c49fb",
   "metadata": {},
   "source": [
    "Multinomial Logistic Regression is a classification algorithm used when the target variable is categorical and has more than two possible classes.   \n",
    "Unlike binary logistic regression, which uses the sigmoid function, multinomial logistic regression uses the softmax function to compute the probability of each class.   \n",
    "\n",
    "In multinomial logistic regression, we model the probability of a data point belonging to one of multiple possible classes. For example, we might predict weather conditions such as \"No Precipitation,\" \"Rain,\" or \"Snowfall.\"   \n",
    "In scikit-learn, multinomial logistic regression by default outputs a predicted class y for each data point, selecting the class with the highest probability. \n",
    "\n",
    "How the Algorithm Works:\n",
    "\n",
    "**Input Features and Coefficients:**   \n",
    "The model takes the input features (independent variables, X) and initializes a vector of coefficients (β) for each class.   \n",
    "These coefficients are initially set to small random values or zeros and will be adjusted during the training process by the solver (the optimization algorithm). \n",
    "\n",
    "**Understanding Logits (Log Odds):**   \n",
    "In logistic regression, a core concept is the logit, also known as the log odds and represents a way of expressing probabilities.   \n",
    "For a given class k, the odds are defined as the ratio of the probability of that class occurring to the probability of it not occurring:   \n",
    "    $$\n",
    "\\text{odds}_k = \\frac{P(y = y_k \\mid X)}{1 - P(y = y_k \\mid X)}\n",
    "    $$\n",
    "\n",
    "And consequently, for a given class k, the logit is defined as the natural logarithm of the odds of that class occurring.     \n",
    "This transformation maps probabilities (which range from 0 to 1) to logits (which range from −∞ to +∞), making it possible to create a linear relationship between the input features and the predicted outcome y.\n",
    "\n",
    "**Logit Calculation:**   \n",
    "For each class k, the logit is computed as a linear combination of the input features and their corresponding coefficients:   \n",
    "    $$\n",
    "\\text{logit}_k = \\beta_{0,k} + \\beta_{1,k} x_1 + \\beta_{2,k} x_2 + \\dots + \\beta_{n,k} x_n\n",
    "    $$\n",
    "\n",
    "The model calculates a logit for each class, and this is the raw score before probabilities are computed. Applying this and you end up in our project with 3 logits for every record in your dataset. (1 logit for the 'no precipitation class/label, 1 logit for the 'rain' class/label and 1 logit for the 'snowfall' class/label) \n",
    "\n",
    "**Softmax Transformation:**   \n",
    "The logits are then passed through the softmax function, which converts them into probabilities for each class.   \n",
    "The softmax function also ensures that all probabilities are between 0 and 1, and that the total probability across all classes equals 1:   \n",
    "    $$\n",
    "   P(y = k \\mid X) = \\frac{\\exp(\\text{logit}_k)}{\\sum_{j=1}^{K} \\exp(\\text{logit}_j)}\n",
    "    $$\n",
    "So after the Softmax transformation every record in the dataset has been assigned with a probability for each class.   \n",
    "In our case the model predicts whether there will be 'No Precipitation', 'Rain', or 'Snowfall', and it calculates three logits (one for each class). These logits are then transformed into probabilities using the softmax function and the model assigns the class with the highest probability to the target variable y.   \n",
    "For example, a record in our dataset could end up having the following probabilities:\n",
    "   - No Precipitation: 0.60   \n",
    "   - Rain: 0.30   \n",
    "   - Snowfall: 0.10\n",
    "   \n",
    "Then the model would predict the class 'No Precipitation' for the target variable y, since it has the highest probability.\n",
    "\n",
    "**Loss/Error Function (Cross-Entropy):**   \n",
    "\n",
    "The cross-entropy loss function measures the error between the predicted probabilities and the true class labels. It’s calculated as:   \n",
    "    $$\n",
    "   L = - \\sum_{k=1}^{K} y_k \\log \\left( P(y = k \\mid X) \\right)\n",
    "    $$   \n",
    "\n",
    "This loss is used as feedback to the model. The closer the predicted probability is to the true label, the smaller the loss.   \n",
    "To make it more concrete we will use the above example where the model predicted 'No precipitation' and suppose that the true label was 'rain'.   \n",
    "In this example, since the true label is Rain, we have $y_{\\text{rain}}=1$, and for the other classes, $y_{\\text{no precipitation}}=0$ and $y_{\\text{snowfall}}=0$.   \n",
    "\n",
    "Applying the formula of the loss function shown above, and considering only the probability for the correct class (Rain), as the other terms will be multiplied by 0, we have: $L = -(1 \\times \\log(0.30)) - (0 \\times \\log(0.60)) - (0 \\times \\log(0.10))$ which leads to:   $L = -\\log(0.30) \\approx -(-0.523) = 0.523$.   \n",
    "This is the loss for this particular record (0.523).\n",
    "\n",
    "\n",
    "At this point the solver takes charge and adjusts the coefficients in the direction to minimize the loss function.\n",
    "\n",
    "**Solver and Coefficient Adjustment:**   \n",
    "The solver is the optimization algorithm responsible for adjusting the β vectors (the coefficients). It iteratively updates the coefficients to reduce the loss, stopping when the change in the loss function between iterations falls below a certain threshold (tol) or after a maximum number of iterations (max_iter) is reached.\n",
    "\n",
    "All the above come into place in the scikit-learn library where the model/algorithm is implemented. You can find all the details you may need in the official documentation of the library **[here](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
